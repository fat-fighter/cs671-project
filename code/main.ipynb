{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS671 - k-Match-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from includes import config\n",
    "from includes.utils import is_outlier\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams = mpl.rc_params_from_file(\"includes/matplotlibrc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Clustering data based on tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Choosing the number of Clusters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "data = []\n",
    "with open(\"data/squad/train.context\") as f:\n",
    "    for line in f:\n",
    "        data.append(line.strip())\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=config.max_df,\n",
    "    min_df=config.min_df,\n",
    "    max_features=config.max_tfidf_vocab,\n",
    "    stop_words='english',\n",
    "    use_idf=True\n",
    ")\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "X_tr = X[:7 * X.shape[0] / 10]\n",
    "X_ts = X[7 * X.shape[0] / 10:]\n",
    "\n",
    "s = 35\n",
    "e = 36\n",
    "\n",
    "km = [\n",
    "    MiniBatchKMeans(\n",
    "        n_init=20,\n",
    "        n_clusters=k,\n",
    "        batch_size=2500,\n",
    "        max_iter=10000,\n",
    "        init='k-means++'\n",
    "    ) for k in range(s, e)\n",
    "]\n",
    "\n",
    "for k in range(0, e - s):\n",
    "    km[k].fit(X_tr)\n",
    "\n",
    "inertias = [km[k].inertia_ for k in range(0, e - s)]\n",
    "plt.plot(range(s, e), inertias)\n",
    "\n",
    "if not os.path.exists(\"plots/\"):\n",
    "    os.makedirs(\"plots/\")\n",
    "plt.savefig()\n",
    "\n",
    "for k in range(0, e - s):\n",
    "    cluster_labels = km[k].predict(X_ts)\n",
    "    silhouette_avg = silhouette_score(X_ts, cluster_labels)\n",
    "    print(\"n_clusters =\", k + s, \", silhouette_score =\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Clustering data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Clustering on the basis of Tf-IDF Representation of\n",
    "## the questions or context as per the value if the\n",
    "## parameter 'clustering' in 'includes/config'.\n",
    "\n",
    "## This idea was dropped later\n",
    "\n",
    "cd = np.concatenate([data, val_data])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=config.max_df,\n",
    "    min_df=config.min_df,\n",
    "    max_features=config.max_tfidf_vocab,\n",
    "    stop_words='english',\n",
    "    use_idf=True,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X = vectorizer.fit_transform(cd)\n",
    "\n",
    "X_val = X[len(data):]\n",
    "X = X[:len(data)]\n",
    "\n",
    "km = KMeans(\n",
    "    n_init=config.kmeans_n_init,\n",
    "    max_iter=config.kmeans_max_iter,\n",
    "    n_clusters=config.n_clusters,\n",
    "    init='k-means++',\n",
    "    n_jobs=-1,\n",
    "    algorithm=\"full\"\n",
    ")\n",
    "\n",
    "km.fit(X)\n",
    "\n",
    "labels = np.array([str(label) for label in km.predict(X)])\n",
    "with open(\"data/squad/train.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(labels))\n",
    "\n",
    "labels_val = np.array([str(label) for label in km.predict(X_val)])\n",
    "with open(\"data/squad/val.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(labels))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "ax1.hist(labels, bins=config.n_clusters, rwidth=0.7)\n",
    "ax2.hist(labels_val, bins=config.n_clusters, rwidth=0.7)\n",
    "plt.show()\n",
    "\n",
    "labels = np.array([int(label) for label in labels])\n",
    "labels_val = np.array([int(label) for label in labels_val])\n",
    "\n",
    "_X = X_val\n",
    "labels = km.predict(_X)\n",
    "_X = _X.toarray()\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(n_components=2)\n",
    "clf.fit(_X, labels)\n",
    "\n",
    "tr_X = clf.transform(_X)\n",
    "\n",
    "filter = ~is_outlier(tr_X)\n",
    "tr_X = tr_X[filter]\n",
    "labels = labels[filter]\n",
    "\n",
    "for label in range(0, config.n_clusters):\n",
    "    label_X = tr_X[labels == label]\n",
    "    plt.scatter(label_X[:, 0], label_X[:, 1], s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Clustering based on question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = ''.join([i if ord(i) < 128 else ' ' for i in text.strip()])\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    text = nltk.Text(tokens)\n",
    "    \n",
    "    return [w.lower() for w in text if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"data/squad/train.question\") as f:\n",
    "    for line in f:\n",
    "        data.append(clean_text(line))\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_data = []\n",
    "with open(\"data/squad/val.question\") as f:\n",
    "    for line in f:\n",
    "        val_data.append(clean_text(line))\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_label(line):\n",
    "    bow = np.zeros(len(line))\n",
    "    words = {\"what\": 0, \"where\": 1, \"who\": 2, \"how\": 3, \"which\": 4}\n",
    "    for word in words:\n",
    "        bow[line == word] = 1\n",
    "    \n",
    "    try:\n",
    "        _label = words[line[np.where(bow == 1)[0][0]]]\n",
    "        return _label\n",
    "    except Exception as e:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels = np.array([get_label(np.array(line)) for line in data])\n",
    "labels_val = np.array([get_label(np.array(line)) for line in val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/squad/train.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(\n",
    "        [str(label) for label in labels]\n",
    "    ))\n",
    "\n",
    "with open(\"data/squad/val.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(\n",
    "        [str(label) for label in labels_val]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "ax1.hist(labels, bins=config.n_clusters, rwidth=0.7)\n",
    "ax2.hist(labels_val, bins=config.n_clusters, rwidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match-LSTM for Machine Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fat-fighter/Softwares/conda/envs/machine-learning/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from includes import config\n",
    "from includes.utils import squad_dataset, evaluate\n",
    "\n",
    "from graph import Graph\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams = mpl.rc_params_from_file(\"includes/matplotlibrc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embedding = np.load(config.embed_path)[\"glove\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    config.encoding_size,\n",
    "    config.dropout_keep_prob\n",
    ")\n",
    "decoder = Decoder(\n",
    "    config.encoding_size,\n",
    "    config.n_clusters,\n",
    "    config.dropout_keep_prob\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From includes/mlstm.py:16: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "graph = Graph(\n",
    "    words_embedding,\n",
    "    encoder,\n",
    "    decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model from model/k-match-lstm/trained_model.chk ... \n",
      "INFO:tensorflow:Restoring parameters from model/k-match-lstm/trained_model.chk\n",
      "Initialized model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = graph.init_model(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = squad_dataset(\n",
    "    config.questions_train,\n",
    "    config.contexts_train,\n",
    "    config.answers_train,\n",
    "    config.labels_train,\n",
    "    root=root_dir + \"/\",\n",
    "    batch_size=config.batch_size\n",
    ")\n",
    "\n",
    "val_data = squad_dataset(\n",
    "    config.questions_val,\n",
    "    config.contexts_val,\n",
    "    config.answers_val,\n",
    "    config.labels_val,\n",
    "    root=root_dir + \"/\",\n",
    "    batch_size=config.val_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(epoch, score):\n",
    "    print \"\\nepoch: %d, f1: %.4f, em: %.4f, em@1: %.4f, em@2: %.4f\\n\" % (\n",
    "        epoch, score[1], score[0][0], score[0][1][0], score[0][1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "if os.path.exists(config.loss_path):\n",
    "    losses = list(np.load(config.loss_path))\n",
    "\n",
    "scores = []\n",
    "if os.path.exists(config.scores_path):\n",
    "    scores = list(np.load(config.scores_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_em = np.max([score[0][1] for score in scores]) or 0\n",
    "\n",
    "if not init:\n",
    "    scores.append(\n",
    "        evaluate(graph, sess, val_data, \"evaluating ... epoch: 0\")\n",
    "    )\n",
    "    print_score(0, scores[-1])\n",
    "else:\n",
    "    score = evaluate(graph, sess, val_data, \"evaluating ... epoch: 0\")\n",
    "    print_score(0, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.num_epochs)[:1]:\n",
    "\n",
    "    losses.append(graph.run_epoch(\n",
    "        train_data, epoch, sess, max_batch_epochs=-1)\n",
    "    )\n",
    "\n",
    "    scores.append(\n",
    "        evaluate(graph, sess, val_data, \"evaluating ... epoch: %d\" % (epoch + 1))\n",
    "    )\n",
    "    print_score(epoch + 1, scores[-1])\n",
    "    \n",
    "    if scores[-1][0][0] >= best_em:\n",
    "        graph.save_model(sess)\n",
    "        best_em = scores[-1][0][0]\n",
    "\n",
    "        np.save(\"data/plots/loss.npy\", np.array(losses))\n",
    "        np.save(\"data/plots/scores.npy\", np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(losses).reshape((4 * len(losses[-1]) / 40, 40)).mean(axis = 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([e[0][0] for e in scores], label=\"em\")\n",
    "plt.plot([e[0][1][0] for e in scores], label=\"em1\")\n",
    "plt.plot([e[0][1][1] for e in scores], label=\"em2\")\n",
    "plt.plot([e[1] for e in scores], label=\"f1\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (Machine Learning)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
