{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS671 - k-Match-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based on Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from includes import config\n",
    "\n",
    "from includes.utils import squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = squad_dataset(\n",
    "    config.question_train,\n",
    "    config.context_train,\n",
    "    config.answer_train,\n",
    "    root=root_dir + \"/\",\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81403"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Match-LSTM for Machine Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from includes import config\n",
    "from includes.utils import squad_dataset, pad_sequences\n",
    "\n",
    "from attention_wrapper import BahdanauAttention, AttentionWrapper\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path):\n",
    "    if os.path.exists(vocab_path):\n",
    "        \n",
    "        with open(vocab_path, mode=\"rb\") as f:\n",
    "            vocab = dict([\n",
    "                (line.strip(), index)\n",
    "                for index, line in enumerate(f.readlines())\n",
    "            ])\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    else:\n",
    "        raise IOError(\"File %s not found.\", vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LSTMCell = tf.contrib.rnn.BasicLSTMCell\n",
    "DynamicRNN = tf.nn.dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CrossEntropy = tf.nn.sparse_softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    LSTM preprocessing  layer to encode the question\n",
    "    and passage representations using a single layer\n",
    "    of LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def encode(self, vectors, lengths):\n",
    "        \"\"\"\n",
    "        vectors  ::  tuple  ::  Word vectors of Question and Passage\n",
    "        lengths  ::  tuple  ::  Word vectors of Question and Passage\n",
    "        \"\"\"\n",
    "        questions, passages = vectors\n",
    "        questions_length, passages_length = lengths\n",
    "        \n",
    "        question_lstm_cell = LSTMCell(\n",
    "            self.hidden_size,\n",
    "            state_is_tuple=True,\n",
    "            name=\"question_lstm_cell\"\n",
    "        )\n",
    "        encoded_questions, (q_rep, _) = DynamicRNN(\n",
    "            question_lstm_cell,\n",
    "            questions,\n",
    "            questions_length,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        passage_lstm_cell = LSTMCell(\n",
    "            self.hidden_size,\n",
    "            state_is_tuple=True,\n",
    "            name=\"passage_lstm_cell\"\n",
    "        )\n",
    "        encoded_passages, (p_rep, _) =  DynamicRNN(\n",
    "            passage_lstm_cell,\n",
    "            passages,\n",
    "            passages_length,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "            \n",
    "        return encoded_questions, q_rep, encoded_passages, p_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MatchEncoder():\n",
    "    \"\"\"\n",
    "    Match-LSTM layer to encode the question\n",
    "    representation in order to get a hidden\n",
    "    representation of the question and the passage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, encoded_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoded_size = encoded_size\n",
    "    \n",
    "    def run_match_lstm(self, vectors, lengths):\n",
    "        questions, passages = vectors\n",
    "        questions_length, passages_length = lengths\n",
    "        \n",
    "        def attention_function(x, state):\n",
    "            return tf.concat([x, state], axis=-1)\n",
    "        \n",
    "        attention_mechanism_match_lstm = BahdanauAttention(\n",
    "            self.encoded_size,\n",
    "            questions,\n",
    "            memory_sequence_length=questions_length\n",
    "        )\n",
    "        \n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "            self.hidden_size, state_is_tuple=True\n",
    "        )\n",
    "        lstm_attender = AttentionWrapper(\n",
    "            cell,\n",
    "            attention_mechanism_match_lstm,\n",
    "            output_attention=False,\n",
    "            attention_input_fn=attention_function\n",
    "        )\n",
    "\n",
    "        reverse_encoded_passage = tf.reverse_sequence(passages, passages_length, batch_axis=0, seq_axis=1)\n",
    "\n",
    "        output_attender_fw, _ = tf.nn.dynamic_rnn(\n",
    "            lstm_attender, passages, dtype=tf.float32\n",
    "        )\n",
    "        output_attender_bw, _ = tf.nn.dynamic_rnn(\n",
    "            lstm_attender, reverse_encoded_passage, dtype=tf.float32, scope=\"rnn\")\n",
    "\n",
    "        output_attender_bw = tf.reverse_sequence(output_attender_bw, passages_length, batch_axis=0, seq_axis=1)\n",
    "        \n",
    "        output_attender = tf.concat(\n",
    "            [output_attender_fw, output_attender_bw], axis=-1\n",
    "        )\n",
    "        return output_attender\n",
    "\n",
    "    def run_answer_pointer(self, output_attender, lengths, labels):\n",
    "        questions_length, passages_length = lengths\n",
    "        labels = tf.unstack(labels, axis=1)\n",
    "\n",
    "        def input_function(curr_input, passage):\n",
    "            return passage\n",
    "        \n",
    "        query_depth_answer_ptr = output_attender.get_shape()[-1]\n",
    "\n",
    "        with tf.variable_scope(\"answer_ptr_attender\"):\n",
    "            attention_mechanism_answer_ptr = BahdanauAttention(\n",
    "                query_depth_answer_ptr,\n",
    "                output_attender,\n",
    "                memory_sequence_length=passages_length\n",
    "            )\n",
    "            \n",
    "            cell_answer_ptr = tf.contrib.rnn.BasicLSTMCell(\n",
    "                self.hidden_size, state_is_tuple=True)\n",
    "            answer_ptr_attender = AttentionWrapper(\n",
    "                cell_answer_ptr, attention_mechanism_answer_ptr, cell_input_fn=input_function)\n",
    "            logits, _ = tf.nn.static_rnn(\n",
    "                answer_ptr_attender, labels, dtype=tf.float32)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, vectors, lengths, questions_representation, labels):\n",
    "        output_attender = self.run_match_lstm(vectors, lengths)\n",
    "        logits = self.run_answer_pointer(output_attender, lengths, labels)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self, encoded_size, match_encoded_size, embeddings):\n",
    "        self.encoded_size = encoded_size\n",
    "        self.match_encoded_size = match_encoded_size\n",
    "        \n",
    "        self.encoder = Encoder(self.encoded_size)\n",
    "        self.model = MatchEncoder(self.match_encoded_size, self.encoded_size)\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.init_placeholders()\n",
    "        self.init_variables()\n",
    "        self.init_nodes()\n",
    "    \n",
    "    def init_placeholders(self):\n",
    "        self.question_ids = tf.placeholder(\n",
    "            tf.int32, shape=[None, None]\n",
    "        )\n",
    "        self.passage_ids = tf.placeholder(\n",
    "            tf.int32, shape=[None, None]\n",
    "        )\n",
    "        self.questions_length = tf.placeholder(\n",
    "            tf.int32, shape=[None]\n",
    "        )\n",
    "        self.passages_length = tf.placeholder(\n",
    "            tf.int32, shape=[None]\n",
    "        )\n",
    "        self.labels = tf.placeholder(\n",
    "            tf.int32, shape=[None, 2]\n",
    "        )\n",
    "        self.dropout = tf.placeholder(\n",
    "            tf.float32, shape=[]\n",
    "        )\n",
    "        \n",
    "    def init_variables(self):\n",
    "        word_embeddings = tf.Variable(\n",
    "            self.embeddings, dtype=tf.float32, trainable=config.train_embeddings\n",
    "        )\n",
    "        questions_embedding = tf.nn.embedding_lookup(\n",
    "            word_embeddings,\n",
    "            self.question_ids\n",
    "        )\n",
    "        passages_embedding = tf.nn.embedding_lookup(\n",
    "            word_embeddings,\n",
    "            self.passage_ids\n",
    "        )\n",
    "        \n",
    "        self.questions = tf.nn.dropout(questions_embedding, self.dropout)\n",
    "        self.passages = tf.nn.dropout(passages_embedding, self.dropout)\n",
    "        \n",
    "    def init_nodes(self):\n",
    "        self.encoded_questions, \\\n",
    "        self.questions_representation, \\\n",
    "        self.encoded_passages, \\\n",
    "        self.passages_representation = self.encoder.encode(\n",
    "            (self.questions, self.passages),\n",
    "            (self.questions_length, self.passages_length)\n",
    "        )\n",
    "\n",
    "        self.logits = self.model.predict(\n",
    "            [self.encoded_questions, self.encoded_passages],\n",
    "            [self.questions_length, self.passages_length],\n",
    "            self.questions_representation,\n",
    "            self.labels\n",
    "        )\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            CrossEntropy(\n",
    "                logits=self.logits[0], labels=self.labels[:, 0]\n",
    "            ) + \\\n",
    "            CrossEntropy(\n",
    "                logits=self.logits[1], labels=self.labels[:, 1]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        adam_optimizer = tf.train.AdamOptimizer()\n",
    "        grads, vars = zip(*adam_optimizer.compute_gradients(self.loss))\n",
    "\n",
    "        self.gradients = zip(grads, vars)\n",
    "\n",
    "        self.train_step = adam_optimizer.apply_gradients(self.gradients)\n",
    "        \n",
    "    def test(self, session, valid):\n",
    "        q, c, a = valid\n",
    "\n",
    "        # at test time we do not perform dropout.\n",
    "        padded_questions, questions_length = pad_sequences(q, 0)\n",
    "        padded_passages, passages_length = pad_sequences(c, 0)\n",
    "        \n",
    "        input_feed={\n",
    "            self.question_ids: np.array(padded_questions),\n",
    "            self.passage_ids: np.array(padded_passages),\n",
    "            self.questions_length: np.array(questions_length),\n",
    "            self.passages_length: np.array(passages_length),\n",
    "            self.labels: np.array(a),\n",
    "            self.dropout: config.train_dropout_val\n",
    "        }\n",
    "\n",
    "        output_feed = [self.logits]\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "\n",
    "        return outputs[0][0], outputs[0][1]\n",
    "        \n",
    "    def answer(self, session, dataset):\n",
    "        yp, yp2 = self.test(session, dataset)\n",
    "        \n",
    "        def func(y1, y2):\n",
    "            max_ans = -999999\n",
    "            a_s, a_e= 0,0\n",
    "            num_classes = len(y1)\n",
    "            for i in xrange(num_classes):\n",
    "                for j in xrange(15):\n",
    "                    if i+j >= num_classes:\n",
    "                        break\n",
    "\n",
    "                    curr_a_s = y1[i];\n",
    "                    curr_a_e = y2[i+j]\n",
    "                    if (curr_a_e+curr_a_s) > max_ans:\n",
    "                        max_ans = curr_a_e + curr_a_s\n",
    "                        a_s = i\n",
    "                        a_e = i+j\n",
    "\n",
    "            return (a_s, a_e)\n",
    "\n",
    "\n",
    "        a_s, a_e = [], []\n",
    "        for i in xrange(yp.shape[0]):\n",
    "            _a_s, _a_e = func(yp[i], yp2[i])\n",
    "            a_s.append(_a_s)\n",
    "            a_e.append(_a_e)\n",
    " \n",
    "\n",
    "        return (np.array(a_s), np.array(a_e))\n",
    "\n",
    "\n",
    "    def evaluate_model(self, session, dataset):\n",
    "        \n",
    "        q, c, a = zip(*[_row[0] for _row in dataset])\n",
    "\n",
    "        sample = len(dataset)\n",
    "        a_s, a_o = self.answer(session, [q, c, a])\n",
    "        answers = np.hstack([a_s.reshape([sample, -1]), a_o.reshape([sample,-1])])\n",
    "        gold_answers = np.array([a[0][2] for a in dataset])\n",
    "\n",
    "        em_score = 0\n",
    "        em_1 = 0\n",
    "        em_2 = 0\n",
    "        for i in xrange(sample):\n",
    "            gold_s, gold_e = gold_answers[i]\n",
    "            s, e = answers[i]\n",
    "            if (s==gold_s): em_1 += 1.0\n",
    "            if (e==gold_e): em_2 += 1.0\n",
    "            if (s == gold_s and e == gold_e):\n",
    "                em_score += 1.0\n",
    "\n",
    "        em_1 /= float(len(answers))\n",
    "        em_2 /= float(len(answers))\n",
    "        print(\"\\nExact match on 1st token: %5.4f | Exact match on 2nd token: %5.4f\\n\" %(em_1, em_2))\n",
    "\n",
    "        em_score /= float(len(answers))\n",
    "\n",
    "        return em_score\n",
    "\n",
    "    def train(self, train_dataset, val_dataset):\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run(session=sess)\n",
    "            \n",
    "            print_dict = {\"loss\": \"inf\"}\n",
    "            for epoch in range(config.num_epochs):\n",
    "                with tqdm(train_data, postfix=print_dict) as pbar:\n",
    "                    pbar.set_description(\"Epoch %d\" % (epoch + 1))\n",
    "\n",
    "                    index = 0\n",
    "                    for batch in pbar:\n",
    "                        padded_questions, questions_length = pad_sequences(np.array(batch[:, 0]), 0)\n",
    "                        padded_passages, passages_length = pad_sequences(np.array(batch[:, 1]), 0)\n",
    "\n",
    "                        loss, _ = sess.run(\n",
    "                            [self.loss, self.train_step],\n",
    "                            feed_dict={\n",
    "                                self.question_ids: np.array(padded_questions),\n",
    "                                self.passage_ids: np.array(padded_passages),\n",
    "                                self.questions_length: np.array(questions_length),\n",
    "                                self.passages_length: np.array(passages_length),\n",
    "                                self.labels: np.array([np.array(el[2]) for el in batch]),\n",
    "                                self.dropout: config.train_dropout_val\n",
    "                            }\n",
    "                        )\n",
    "                        print_dict[\"loss\"] = \"%.3f\" % loss\n",
    "                        pbar.set_postfix(print_dict)\n",
    "                        if index == 5:\n",
    "                            break\n",
    "                        index += 1\n",
    "                        \n",
    "                em = self.evaluate_model(sess, val_dataset)\n",
    "                print(\"\\n#-----------Exact match on val set: %5.4f #-----------\\n\" %em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words_embedding = np.load(config.embed_path)[\"glove\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = squad_dataset(\n",
    "    config.question_train,\n",
    "    config.context_train,\n",
    "    config.answer_train,\n",
    "    root=root_dir + \"/\",\n",
    "    batch_size=config.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_data = squad_dataset(\n",
    "    config.question_val,\n",
    "    config.context_val,\n",
    "    config.answer_val,\n",
    "    root=root_dir + \"/\",\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "graph = Graph(config.hidden_state_size, config.hidden_state_size, words_embedding)\n",
    "graph.train(train_data, val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (Machine Learning)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
