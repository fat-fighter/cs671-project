{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS671 - k-Match-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from includes import config\n",
    "from includes.utils import is_outlier\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams = mpl.rc_params_from_file(\"includes/matplotlibrc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Clustering data based on tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Choosing the number of Clusters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "data = []\n",
    "with open(\"data/squad/train.context\") as f:\n",
    "    for line in f:\n",
    "        data.append(line.strip())\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=config.max_df,\n",
    "    min_df=config.min_df,\n",
    "    max_features=config.max_tfidf_vocab,\n",
    "    stop_words='english',\n",
    "    use_idf=True\n",
    ")\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "X_tr = X[:7 * X.shape[0] / 10]\n",
    "X_ts = X[7 * X.shape[0] / 10:]\n",
    "\n",
    "s = 35\n",
    "e = 36\n",
    "\n",
    "km = [\n",
    "    MiniBatchKMeans(\n",
    "        n_init=20,\n",
    "        n_clusters=k,\n",
    "        batch_size=2500,\n",
    "        max_iter=10000,\n",
    "        init='k-means++'\n",
    "    ) for k in range(s, e)\n",
    "]\n",
    "\n",
    "for k in range(0, e - s):\n",
    "    km[k].fit(X_tr)\n",
    "\n",
    "inertias = [km[k].inertia_ for k in range(0, e - s)]\n",
    "plt.plot(range(s, e), inertias)\n",
    "\n",
    "if not os.path.exists(\"plots/\"):\n",
    "    os.makedirs(\"plots/\")\n",
    "plt.savefig()\n",
    "\n",
    "for k in range(0, e - s):\n",
    "    cluster_labels = km[k].predict(X_ts)\n",
    "    silhouette_avg = silhouette_score(X_ts, cluster_labels)\n",
    "    print(\"n_clusters =\", k + s, \", silhouette_score =\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Clustering data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Clustering on the basis of Tf-IDF Representation of\n",
    "## the questions or context as per the value if the\n",
    "## parameter 'clustering' in 'includes/config'.\n",
    "\n",
    "## This idea was dropped later\n",
    "\n",
    "cd = np.concatenate([data, val_data])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=config.max_df,\n",
    "    min_df=config.min_df,\n",
    "    max_features=config.max_tfidf_vocab,\n",
    "    stop_words='english',\n",
    "    use_idf=True,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X = vectorizer.fit_transform(cd)\n",
    "\n",
    "X_val = X[len(data):]\n",
    "X = X[:len(data)]\n",
    "\n",
    "km = KMeans(\n",
    "    n_init=config.kmeans_n_init,\n",
    "    max_iter=config.kmeans_max_iter,\n",
    "    n_clusters=config.n_clusters,\n",
    "    init='k-means++',\n",
    "    n_jobs=-1,\n",
    "    algorithm=\"full\"\n",
    ")\n",
    "\n",
    "km.fit(X)\n",
    "\n",
    "labels = np.array([str(label) for label in km.predict(X)])\n",
    "with open(\"data/squad/train.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(labels))\n",
    "\n",
    "labels_val = np.array([str(label) for label in km.predict(X_val)])\n",
    "with open(\"data/squad/val.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(labels))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "ax1.hist(labels, bins=config.n_clusters, rwidth=0.7)\n",
    "ax2.hist(labels_val, bins=config.n_clusters, rwidth=0.7)\n",
    "plt.show()\n",
    "\n",
    "labels = np.array([int(label) for label in labels])\n",
    "labels_val = np.array([int(label) for label in labels_val])\n",
    "\n",
    "_X = X_val\n",
    "labels = km.predict(_X)\n",
    "_X = _X.toarray()\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(n_components=2)\n",
    "clf.fit(_X, labels)\n",
    "\n",
    "tr_X = clf.transform(_X)\n",
    "\n",
    "filter = ~is_outlier(tr_X)\n",
    "tr_X = tr_X[filter]\n",
    "labels = labels[filter]\n",
    "\n",
    "for label in range(0, config.n_clusters):\n",
    "    label_X = tr_X[labels == label]\n",
    "    plt.scatter(label_X[:, 0], label_X[:, 1], s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Clustering based on question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = ''.join([i if ord(i) < 128 else ' ' for i in text.strip()])\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    text = nltk.Text(tokens)\n",
    "    \n",
    "    return [w.lower() for w in text if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"data/squad/train.question\") as f:\n",
    "    for line in f:\n",
    "        data.append(clean_text(line))\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_data = []\n",
    "with open(\"data/squad/val.question\") as f:\n",
    "    for line in f:\n",
    "        val_data.append(clean_text(line))\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_label(line):\n",
    "    bow = np.zeros(len(line))\n",
    "    words = {\"what\": 0, \"where\": 1, \"who\": 2, \"how\": 3, \"which\": 4}\n",
    "    for word in words:\n",
    "        bow[line == word] = 1\n",
    "    \n",
    "    try:\n",
    "        _label = words[line[np.where(bow == 1)[0][0]]]\n",
    "        return _label\n",
    "    except Exception as e:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels = np.array([get_label(np.array(line)) for line in data])\n",
    "labels_val = np.array([get_label(np.array(line)) for line in val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/squad/train.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(\n",
    "        [str(label) for label in labels]\n",
    "    ))\n",
    "\n",
    "with open(\"data/squad/val.labels\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(\n",
    "        [str(label) for label in labels_val]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "ax1.hist(labels, bins=config.n_clusters, rwidth=0.7)\n",
    "ax2.hist(labels_val, bins=config.n_clusters, rwidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Match-LSTM for Machine Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from includes import config\n",
    "from includes.utils import squad_dataset, pad_sequences\n",
    "\n",
    "from attention_wrapper import BahdanauAttention, AttentionWrapper\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path):\n",
    "    if os.path.exists(vocab_path):\n",
    "        \n",
    "        with open(vocab_path, mode=\"rb\") as f:\n",
    "            vocab = dict([\n",
    "                (line.strip(), index)\n",
    "                for index, line in enumerate(f.readlines())\n",
    "            ])\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    else:\n",
    "        raise IOError(\"File %s not found.\", vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LSTMCell = tf.contrib.rnn.BasicLSTMCell\n",
    "DynamicRNN = tf.nn.dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CrossEntropy = tf.nn.sparse_softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    LSTM preprocessing  layer to encode the question\n",
    "    and passage representations using a single layer\n",
    "    of LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def encode(self, vectors, lengths):\n",
    "        \"\"\"\n",
    "        vectors  ::  tuple  ::  Word vectors of Question and Passage\n",
    "        lengths  ::  tuple  ::  Word vectors of Question and Passage\n",
    "        \"\"\"\n",
    "        questions, passages = vectors\n",
    "        questions_length, passages_length = lengths\n",
    "        \n",
    "        question_lstm_cell = LSTMCell(\n",
    "            self.hidden_size,\n",
    "            state_is_tuple=True,\n",
    "            name=\"question_lstm_cell\"\n",
    "        )\n",
    "        encoded_questions, (q_rep, _) = DynamicRNN(\n",
    "            question_lstm_cell,\n",
    "            questions,\n",
    "            questions_length,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        passage_lstm_cell = LSTMCell(\n",
    "            self.hidden_size,\n",
    "            state_is_tuple=True,\n",
    "            name=\"passage_lstm_cell\"\n",
    "        )\n",
    "        encoded_passages, (p_rep, _) =  DynamicRNN(\n",
    "            passage_lstm_cell,\n",
    "            passages,\n",
    "            passages_length,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "            \n",
    "        return encoded_questions, q_rep, encoded_passages, p_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MatchEncoder():\n",
    "    \"\"\"\n",
    "    Match-LSTM layer to encode the question\n",
    "    representation in order to get a hidden\n",
    "    representation of the question and the passage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, encoded_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoded_size = encoded_size\n",
    "    \n",
    "    def run_match_lstm(self, vectors, lengths):\n",
    "        questions, passages = vectors\n",
    "        questions_length, passages_length = lengths\n",
    "        \n",
    "        def attention_function(x, state):\n",
    "            return tf.concat([x, state], axis=-1)\n",
    "        \n",
    "        attention_mechanism_match_lstm = BahdanauAttention(\n",
    "            self.encoded_size,\n",
    "            questions,\n",
    "            memory_sequence_length=questions_length\n",
    "        )\n",
    "        \n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "            self.hidden_size, state_is_tuple=True\n",
    "        )\n",
    "        lstm_attender = AttentionWrapper(\n",
    "            cell,\n",
    "            attention_mechanism_match_lstm,\n",
    "            output_attention=False,\n",
    "            attention_input_fn=attention_function\n",
    "        )\n",
    "\n",
    "        reverse_encoded_passage = tf.reverse_sequence(passages, passages_length, batch_axis=0, seq_axis=1)\n",
    "\n",
    "        output_attender_fw, _ = tf.nn.dynamic_rnn(\n",
    "            lstm_attender, passages, dtype=tf.float32\n",
    "        )\n",
    "        output_attender_bw, _ = tf.nn.dynamic_rnn(\n",
    "            lstm_attender, reverse_encoded_passage, dtype=tf.float32, scope=\"rnn\")\n",
    "\n",
    "        output_attender_bw = tf.reverse_sequence(output_attender_bw, passages_length, batch_axis=0, seq_axis=1)\n",
    "        \n",
    "        output_attender = tf.concat(\n",
    "            [output_attender_fw, output_attender_bw], axis=-1\n",
    "        )\n",
    "        return output_attender\n",
    "\n",
    "    def run_answer_pointer(self, output_attender, lengths, labels):\n",
    "        questions_length, passages_length = lengths\n",
    "        labels = tf.unstack(labels, axis=1)\n",
    "\n",
    "        def input_function(curr_input, passage):\n",
    "            return passage\n",
    "        \n",
    "        query_depth_answer_ptr = output_attender.get_shape()[-1]\n",
    "\n",
    "        with tf.variable_scope(\"answer_ptr_attender\"):\n",
    "            attention_mechanism_answer_ptr = BahdanauAttention(\n",
    "                query_depth_answer_ptr,\n",
    "                output_attender,\n",
    "                memory_sequence_length=passages_length\n",
    "            )\n",
    "            \n",
    "            cell_answer_ptr = tf.contrib.rnn.BasicLSTMCell(\n",
    "                self.hidden_size, state_is_tuple=True)\n",
    "            answer_ptr_attender = AttentionWrapper(\n",
    "                cell_answer_ptr, attention_mechanism_answer_ptr, cell_input_fn=input_function)\n",
    "            logits, _ = tf.nn.static_rnn(\n",
    "                answer_ptr_attender, labels, dtype=tf.float32)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, vectors, lengths, questions_representation, labels):\n",
    "        output_attender = self.run_match_lstm(vectors, lengths)\n",
    "        logits = self.run_answer_pointer(output_attender, lengths, labels)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self, encoded_size, match_encoded_size, embeddings):\n",
    "        self.encoded_size = encoded_size\n",
    "        self.match_encoded_size = match_encoded_size\n",
    "        \n",
    "        self.encoder = Encoder(self.encoded_size)\n",
    "        self.model = MatchEncoder(self.match_encoded_size, self.encoded_size)\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.init_placeholders()\n",
    "        self.init_variables()\n",
    "        self.init_nodes()\n",
    "    \n",
    "    def init_placeholders(self):\n",
    "        self.question_ids = tf.placeholder(\n",
    "            tf.int32, shape=[None, None]\n",
    "        )\n",
    "        self.passage_ids = tf.placeholder(\n",
    "            tf.int32, shape=[None, None]\n",
    "        )\n",
    "        self.questions_length = tf.placeholder(\n",
    "            tf.int32, shape=[None]\n",
    "        )\n",
    "        self.passages_length = tf.placeholder(\n",
    "            tf.int32, shape=[None]\n",
    "        )\n",
    "        self.labels = tf.placeholder(\n",
    "            tf.int32, shape=[None, 2]\n",
    "        )\n",
    "        self.dropout = tf.placeholder(\n",
    "            tf.float32, shape=[]\n",
    "        )\n",
    "        \n",
    "    def init_variables(self):\n",
    "        word_embeddings = tf.Variable(\n",
    "            self.embeddings, dtype=tf.float32, trainable=config.train_embeddings\n",
    "        )\n",
    "        questions_embedding = tf.nn.embedding_lookup(\n",
    "            word_embeddings,\n",
    "            self.question_ids\n",
    "        )\n",
    "        passages_embedding = tf.nn.embedding_lookup(\n",
    "            word_embeddings,\n",
    "            self.passage_ids\n",
    "        )\n",
    "        \n",
    "        self.questions = tf.nn.dropout(questions_embedding, self.dropout)\n",
    "        self.passages = tf.nn.dropout(passages_embedding, self.dropout)\n",
    "        \n",
    "    def init_nodes(self):\n",
    "        self.encoded_questions, \\\n",
    "        self.questions_representation, \\\n",
    "        self.encoded_passages, \\\n",
    "        self.passages_representation = self.encoder.encode(\n",
    "            (self.questions, self.passages),\n",
    "            (self.questions_length, self.passages_length)\n",
    "        )\n",
    "\n",
    "        self.logits = self.model.predict(\n",
    "            [self.encoded_questions, self.encoded_passages],\n",
    "            [self.questions_length, self.passages_length],\n",
    "            self.questions_representation,\n",
    "            self.labels\n",
    "        )\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            CrossEntropy(\n",
    "                logits=self.logits[0], labels=self.labels[:, 0]\n",
    "            ) + \\\n",
    "            CrossEntropy(\n",
    "                logits=self.logits[1], labels=self.labels[:, 1]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        adam_optimizer = tf.train.AdamOptimizer()\n",
    "        grads, vars = zip(*adam_optimizer.compute_gradients(self.loss))\n",
    "\n",
    "        self.gradients = zip(grads, vars)\n",
    "\n",
    "        self.train_step = adam_optimizer.apply_gradients(self.gradients)\n",
    "        \n",
    "    def test(self, session, valid):\n",
    "        q, c, a = valid\n",
    "\n",
    "        # at test time we do not perform dropout.\n",
    "        padded_questions, questions_length = pad_sequences(q, 0)\n",
    "        padded_passages, passages_length = pad_sequences(c, 0)\n",
    "        \n",
    "        input_feed={\n",
    "            self.question_ids: np.array(padded_questions),\n",
    "            self.passage_ids: np.array(padded_passages),\n",
    "            self.questions_length: np.array(questions_length),\n",
    "            self.passages_length: np.array(passages_length),\n",
    "            self.labels: np.array(a),\n",
    "            self.dropout: config.train_dropout_val\n",
    "        }\n",
    "\n",
    "        output_feed = [self.logits]\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "\n",
    "        return outputs[0][0], outputs[0][1]\n",
    "        \n",
    "    def answer(self, session, dataset):\n",
    "        yp, yp2 = self.test(session, dataset)\n",
    "        \n",
    "        def func(y1, y2):\n",
    "            max_ans = -999999\n",
    "            a_s, a_e= 0,0\n",
    "            num_classes = len(y1)\n",
    "            for i in xrange(num_classes):\n",
    "                for j in xrange(15):\n",
    "                    if i+j >= num_classes:\n",
    "                        break\n",
    "\n",
    "                    curr_a_s = y1[i];\n",
    "                    curr_a_e = y2[i+j]\n",
    "                    if (curr_a_e+curr_a_s) > max_ans:\n",
    "                        max_ans = curr_a_e + curr_a_s\n",
    "                        a_s = i\n",
    "                        a_e = i+j\n",
    "\n",
    "            return (a_s, a_e)\n",
    "\n",
    "\n",
    "        a_s, a_e = [], []\n",
    "        for i in xrange(yp.shape[0]):\n",
    "            _a_s, _a_e = func(yp[i], yp2[i])\n",
    "            a_s.append(_a_s)\n",
    "            a_e.append(_a_e)\n",
    " \n",
    "\n",
    "        return (np.array(a_s), np.array(a_e))\n",
    "\n",
    "\n",
    "    def evaluate_model(self, session, dataset):\n",
    "        \n",
    "        q, c, a = zip(*[_row[0] for _row in dataset])\n",
    "\n",
    "        sample = len(dataset)\n",
    "        a_s, a_o = self.answer(session, [q, c, a])\n",
    "        answers = np.hstack([a_s.reshape([sample, -1]), a_o.reshape([sample,-1])])\n",
    "        gold_answers = np.array([a[0][2] for a in dataset])\n",
    "\n",
    "        em_score = 0\n",
    "        em_1 = 0\n",
    "        em_2 = 0\n",
    "        for i in xrange(sample):\n",
    "            gold_s, gold_e = gold_answers[i]\n",
    "            s, e = answers[i]\n",
    "            if (s==gold_s): em_1 += 1.0\n",
    "            if (e==gold_e): em_2 += 1.0\n",
    "            if (s == gold_s and e == gold_e):\n",
    "                em_score += 1.0\n",
    "\n",
    "        em_1 /= float(len(answers))\n",
    "        em_2 /= float(len(answers))\n",
    "        print(\"\\nExact match on 1st token: %5.4f | Exact match on 2nd token: %5.4f\\n\" %(em_1, em_2))\n",
    "\n",
    "        em_score /= float(len(answers))\n",
    "\n",
    "        return em_score\n",
    "\n",
    "    def train(self, train_dataset, val_dataset):\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run(session=sess)\n",
    "            \n",
    "            print_dict = {\"loss\": \"inf\"}\n",
    "            for epoch in range(config.num_epochs):\n",
    "                with tqdm(train_data, postfix=print_dict) as pbar:\n",
    "                    pbar.set_description(\"Epoch %d\" % (epoch + 1))\n",
    "\n",
    "                    index = 0\n",
    "                    for batch in pbar:\n",
    "                        padded_questions, questions_length = pad_sequences(np.array(batch[:, 0]), 0)\n",
    "                        padded_passages, passages_length = pad_sequences(np.array(batch[:, 1]), 0)\n",
    "\n",
    "                        loss, _ = sess.run(\n",
    "                            [self.loss, self.train_step],\n",
    "                            feed_dict={\n",
    "                                self.question_ids: np.array(padded_questions),\n",
    "                                self.passage_ids: np.array(padded_passages),\n",
    "                                self.questions_length: np.array(questions_length),\n",
    "                                self.passages_length: np.array(passages_length),\n",
    "                                self.labels: np.array([np.array(el[2]) for el in batch]),\n",
    "                                self.dropout: config.train_dropout_val\n",
    "                            }\n",
    "                        )\n",
    "                        print_dict[\"loss\"] = \"%.3f\" % loss\n",
    "                        pbar.set_postfix(print_dict)\n",
    "                        if index == 5:\n",
    "                            break\n",
    "                        index += 1\n",
    "                        \n",
    "                em = self.evaluate_model(sess, val_dataset)\n",
    "                print(\"\\n#-----------Exact match on val set: %5.4f #-----------\\n\" %em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words_embedding = np.load(config.embed_path)[\"glove\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = squad_dataset(\n",
    "    config.question_train,\n",
    "    config.context_train,\n",
    "    config.answer_train,\n",
    "    root=root_dir + \"/\",\n",
    "    batch_size=config.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_data = squad_dataset(\n",
    "    config.question_val,\n",
    "    config.context_val,\n",
    "    config.answer_val,\n",
    "    root=root_dir + \"/\",\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "graph = Graph(config.hidden_state_size, config.hidden_state_size, words_embedding)\n",
    "graph.train(train_data, val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (Machine Learning)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
